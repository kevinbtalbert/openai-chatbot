max tegmark:
nd it's difficult for two separate reasons. There's the technical value alignment problem of figuring out just how to make machines understand their goals, adopt them and retain them. And then there's the separate part of it, the philosophical part whose values anyway. And since we, it's not like we have any great consensus on this planet on values, how, what mechanism should we create then to aggregate and decide? OK, what's a good compromise? Uh That second discussion can't just be left the tech nerds like myself, right? That's right. And if we refuse to talk about it and then a G I gets built, who's gonna be actually making the decision about whose values? It's gonna be a bunch of dudes and some tech company, right? And are they necessarily so so representative of all of humankind that we want to just entrust it to them? Are they even uniquely qualified to speak to future human happiness just because they're good at programming. A, I, I'd much rather have this be a really inclusive conversation. But do you